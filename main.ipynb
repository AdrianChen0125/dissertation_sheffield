{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b1e812",
   "metadata": {},
   "source": [
    "# Topic Analysing Public Perception of Mr. Beast's Controversial Videos on YouTube Through Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce49a0b",
   "metadata": {},
   "source": [
    "## 1.Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f62a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string,emoji\n",
    "from cleantext import clean\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "pd.set_option(\"mode.copy_on_write\", False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c4fb3",
   "metadata": {},
   "source": [
    "## 2.Read data and select target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d25d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True \n",
    "# read data into dataframe\n",
    "Yt_comments = pd.read_csv(r'./YouTube_comments_20240616.txt',sep='\\t')\n",
    "\n",
    "# Select Target column  \n",
    "selected_columns=['VideoID',\n",
    "                  'CommentPublished',\n",
    "                  'CommentTextDisplay',\n",
    "                  'CommentAuthorName',\n",
    "                  'CommentLikeCount']\n",
    "Yt_comments = Yt_comments[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8e7dd",
   "metadata": {},
   "source": [
    "## 3. Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data structure \n",
    "Yt_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data type and null value\n",
    "print(\"Information of Data set\")\n",
    "Yt_comments.info()\n",
    "\n",
    "# Check null value \n",
    "print(Yt_comments.isnull().sum())\n",
    "\n",
    "# Remove missing value\n",
    "Yt_comments = Yt_comments.dropna()\n",
    "\n",
    "# Change data type \n",
    "Yt_comments['CommentPublished'] = pd.to_datetime(Yt_comments['CommentPublished'])\n",
    "Yt_comments['CommentPublished_YM'] = Yt_comments['CommentPublished'].dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e6ff2",
   "metadata": {},
   "source": [
    "### 3.1 Set filter on date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4867fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualisation about variations of comments from 2023-11 to 2024-06\n",
    "gb_date = (Yt_comments\n",
    "           .groupby('CommentPublished_YM')['CommentTextDisplay']\n",
    "           .agg('count')\n",
    "           .reset_index(name=\"count_comments\"))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(gb_date['CommentPublished_YM'], \n",
    "        gb_date[\"count_comments\"], \n",
    "        color='skyblue')\n",
    "\n",
    "plt.title('Count of comments by year and month')\n",
    "plt.xlabel('Year-month')\n",
    "plt.ylabel('Count of comments')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "for index, value in enumerate(gb_date[\"count_comments\"]):\n",
    "    plt.text(index, value + 0.5, str(value), ha='center')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Create visualisation about variations of comments in 2023-11 \n",
    " \n",
    "Yt_comments = Yt_comments[Yt_comments['CommentPublished_YM']=='2023-11'] \n",
    "\n",
    "Yt_comments['CommentPublished_D'] = Yt_comments['CommentPublished'].dt.strftime('%d')\n",
    "\n",
    "gb_date_cms = (Yt_comments\n",
    "               .groupby('CommentPublished_D')['CommentTextDisplay']\n",
    "               .agg('count')\n",
    "               .reset_index(name='count_comments')\n",
    "               )\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(gb_date_cms['CommentPublished_D'],gb_date_cms['count_comments'],marker='o', linestyle='--')\n",
    "plt.title('Count of comments by day in Nov-2023')\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('Count of comments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c75071",
   "metadata": {},
   "source": [
    "### 3.2 Text Preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1686e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to detect non-englis word from comments \n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(str(text)) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "  \n",
    "Yt_comments['is_eng'] = Yt_comments['CommentTextDisplay'].apply(is_english)\n",
    "\n",
    "# Creatate visualisation to Check percentage of Non-English comments\n",
    "\n",
    "eng_valid_cms = (Yt_comments\n",
    "                 .groupby('is_eng')['CommentTextDisplay']\n",
    "                 .agg('count')\n",
    "                 .reset_index(name='count_comments')\n",
    "                 )\n",
    "\n",
    "eng_valid_cms['Percentage'] = (eng_valid_cms[\"count_comments\"] / eng_valid_cms[\"count_comments\"].sum()) * 100\n",
    "\n",
    "plt.pie(\n",
    "    eng_valid_cms['Percentage'],\n",
    "    labels= eng_valid_cms['is_eng'], \n",
    "    autopct=lambda p: f'{p:.1f}%',\n",
    "    startangle=140\n",
    "    )\n",
    "\n",
    "plt.show() \n",
    "\n",
    "# Remove Non-english comments\n",
    "Yt_comments_eng_cms = Yt_comments[Yt_comments['is_eng']]\n",
    "removed_num = Yt_comments['CommentTextDisplay'].count()-Yt_comments_eng_cms['CommentTextDisplay'].count()\n",
    "print(f\"Remove {removed_num} rows, left {Yt_comments_eng_cms['CommentTextDisplay'].count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URL and user name \n",
    "def remove_urls_usernames(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove usernames (assuming they start with '@')\n",
    "    text = re.sub(r'@\\w+', '', text)  \n",
    "    return text\n",
    "\n",
    "# Converts text to lowercase.\n",
    "def to_lowercase(text):    \n",
    "    return text.lower()\n",
    "\n",
    "# Removes punctuation from text.\n",
    "def rm_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Removes emojis from text.\n",
    "def remove_emoji(text):\n",
    "    return clean(text, no_emoji=True)  \n",
    "\n",
    "# Tokenizes text into words.\n",
    "def tokenise(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "#  Removes stop words.\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Lemmatizes tokens\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Rejoins tokens back into a string.\n",
    "def rejoin_tokens(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocesses pipeline for Topic modelling.\n",
    "def preprocess_text_nmf(text):\n",
    "    text = remove_urls_usernames(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = rm_punctuation(text)\n",
    "    text = remove_emoji(text)  \n",
    "    tokens = tokenise(text)\n",
    "    tokens = lemmatize(tokens)  \n",
    "    tokens = remove_stopwords(tokens)\n",
    "    return rejoin_tokens(tokens)\n",
    "\n",
    "# Preprocesses pipeline for Vader.\n",
    "def preprocess_text_vader(text):\n",
    "    text = remove_urls_usernames(text)\n",
    "    return text \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c50a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_process text for NMF\n",
    "Yt_comments_eng_cms['preprocessed_text_nmf'] = Yt_comments_eng_cms['CommentTextDisplay'].apply(preprocess_text_nmf)\n",
    "\n",
    "# pre_process text for VADER\n",
    "Yt_comments_eng_cms['preprocessed_text_vader'] = Yt_comments_eng_cms['CommentTextDisplay'].apply(preprocess_text_vader)\n",
    "\n",
    "# Check there is no null value and space in comments after preprocessing \n",
    "Yt_comments_eng_cms = Yt_comments_eng_cms[(Yt_comments_eng_cms['preprocessed_text_nmf'].notnull())&\n",
    "                                          (Yt_comments_eng_cms['preprocessed_text_nmf']!='')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50867af7",
   "metadata": {},
   "source": [
    "## 4. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34eae1",
   "metadata": {},
   "source": [
    "### 4.1 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5649d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word cloud for frequent word in comments \n",
    "text = \" \".join(Yt_comments_eng_cms['preprocessed_text_nmf']) \n",
    "wordcloud = WordCloud(max_words=20,\n",
    "                      width=800, \n",
    "                      height=400, \n",
    "                      background_color='white', \n",
    "                      min_font_size=10).generate(text)\n",
    "\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742bd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Combine text into a single string\n",
    "all_text = \" \".join(Yt_comments_eng_cms['preprocessed_text_nmf'])\n",
    "\n",
    "# Tokenize and get bigrams\n",
    "bigrams = list(ngrams(all_text.split(), 2))\n",
    "\n",
    "# Count bigram frequencies\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "# Sort bigrams by frequency (descending)\n",
    "sorted_bigrams = sorted(\n",
    "    bigram_counts.items(), \n",
    "    key=lambda item: item[1],\n",
    "    reverse=True\n",
    "    )\n",
    "\n",
    "# Extract bigrams and frequencies for plotting\n",
    "bigrams, frequencies = zip(*sorted_bigrams)\n",
    "\n",
    "# Convert bigrams to strings for x-axis labels\n",
    "bigrams = [\" \".join(bigram) for bigram in bigrams]\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(bigrams[0:15], frequencies[0:15])\n",
    "plt.xlabel('Bigrams')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bigram Frequency Distribution')\n",
    "plt.xticks(rotation=45)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text into a single string\n",
    "all_text = \" \".join(Yt_comments_eng_cms['preprocessed_text_nmf'])\n",
    "\n",
    "# Tokenize and get trigrams\n",
    "trigrams = list(ngrams(all_text.split(), 3))\n",
    "\n",
    "# Count trigram frequencies\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "# Sort trigrams by frequency (descending)\n",
    "sorted_trigrams = sorted(\n",
    "    trigram_counts.items(),\n",
    "    key=lambda item: item[1],\n",
    "    reverse=True\n",
    "    )\n",
    "# Extract trigrams and frequencies for plotting\n",
    "trigrams, frequencies = zip(*sorted_trigrams)\n",
    "\n",
    "# Convert trigrams to strings for x-axis labels\n",
    "trigrams = [\" \".join(trigram) for trigram in trigrams]\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(trigrams[0:10], frequencies[0:10])\n",
    "plt.xlabel('Trigrams')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Trigram Frequency Distribution')\n",
    "plt.xticks(rotation=45)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0124c1",
   "metadata": {},
   "source": [
    "### 4.2 Research Question 1 What potential topics are discussed in the comments of Mr Beast's YouTube video, \"I Built 100 Wells in Africa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319bf217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "# find best number of topic \n",
    "# Get the TF-IDF matrix from your DataFrame\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, \n",
    "                                   min_df=5, \n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(Yt_comments_eng_cms['preprocessed_text_nmf'])\n",
    "\n",
    "# Define the range of topic numbers to explore\n",
    "topic_nums = range(5, 15)  \n",
    "coherence_values = []\n",
    "\n",
    "# Iterate through topic numbers\n",
    "for num_topics in topic_nums:\n",
    "\n",
    "    # Train the NMF model\n",
    "    nmf = NMF(n_components=num_topics,\n",
    "               random_state=1)\n",
    "    \n",
    "    doc_topic = nmf.fit_transform(tfidf_matrix)\n",
    "\n",
    "    # Get the word ids from the vectorizer\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    # Create a dictionary from the feature names\n",
    "    dictionary = Dictionary([tfidf_feature_names])\n",
    "\n",
    "    # Get top words per topic\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        top_words.append([tfidf_feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "    \n",
    "    # Calculate coherence (using 'c_v' measure)\n",
    "    cm = CoherenceModel(topics=top_words, \n",
    "                        texts= Yt_comments_eng_cms['preprocessed_text_nmf'].apply(word_tokenize), \n",
    "                        coherence='c_v', \n",
    "                        dictionary=dictionary)\n",
    "    \n",
    "    coherence_values.append(cm.get_coherence())\n",
    "    \n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(topic_nums, coherence_values)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"C_v Coherence Score\")\n",
    "plt.title(\"NMF Topic Coherence with C_v Measure\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49622277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise the processed text using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=5, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(Yt_comments_eng_cms['preprocessed_text_nmf'])\n",
    "\n",
    "# Apply NMF for topic modeling\n",
    "num_topics = 7\n",
    "\n",
    "nmf_model = NMF(n_components= num_topics,\n",
    "                 random_state=1)\n",
    "\n",
    "nmf_topics = nmf_model.fit_transform(tfidf)\n",
    "\n",
    "# Get the top words for each topic\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return top_words\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "top_words = get_top_words(nmf_model, \n",
    "                          tfidf_feature_names, \n",
    "                          11)\n",
    "\n",
    "# Print the top words for each topic\n",
    "for i, words in enumerate(top_words):\n",
    "    print(f\"Topic {i}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign topics\n",
    "Yt_comments_eng_cms['topic'] = nmf_topics.argmax(axis=1)\n",
    "\n",
    "topic_annotation = {\n",
    "    0:\"People's Needs and Negative Reactions\", \n",
    "    1:\"Opinions about Mr Beast\",\n",
    "    2:\"Moral Judgment of Actions\",\n",
    "    3:\"Access to Clean Water\",\n",
    "    4:\"Racism and Savior Complex\",\n",
    "    5:\"MrBeast's Video and Money for Charity\",\n",
    "    6:\"Sociopolitical Issues and Aid in Africa\"\n",
    "    }\n",
    "\n",
    "Yt_comments_eng_cms['topic_tag'] = Yt_comments_eng_cms['topic'].map(topic_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ã€€Create visualisatiion for interpretation \n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-11 - 1:-1]]\n",
    "    top_weights = [topic[i] for i in topic.argsort()[:-11 - 1:-1]]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))  # Adjust figure size as needed\n",
    "    plt.barh(top_words, top_weights)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.title(f\"{topic_annotation[topic_idx]}_(Topic {topic_idx})\")\n",
    "    plt.xlabel(\"Weights\")\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout for better spacing\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_g_topic = Yt_comments_eng_cms['topic_tag'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.barh(comments_g_topic.index, comments_g_topic.values,color='darkblue')\n",
    "plt.title('Count of Comments by Topic')\n",
    "plt.xlabel('Count of Comments')\n",
    "plt.ylabel('Topic')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa08f48a",
   "metadata": {},
   "source": [
    "### 4.3 Research Question 2 How does the range of audience sentiment vary across the key topics identified (RQ1) in the comments of Mr Beast's video? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c73150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "def categorize_sentiment(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment score\n",
    "Yt_comments_eng_cms['sentiment'] = Yt_comments_eng_cms['preprocessed_text_vader'].apply(analyze_sentiment)\n",
    "# Assign sentiment category \n",
    "Yt_comments_eng_cms['sentiment_category'] = Yt_comments_eng_cms['sentiment'].apply(categorize_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a178bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Sentiments in comments \n",
    "sentiment_counts = Yt_comments_eng_cms['sentiment_category'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(sentiment_counts.index, sentiment_counts.values,color='skyblue')\n",
    "plt.title('The Results of Sentiment Analysis')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count of Comments')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014703f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid value error \n",
    "pd.set_option(\"mode.copy_on_write\", False)\n",
    "sentiment_PN = Yt_comments_eng_cms[Yt_comments_eng_cms['sentiment_category']!='neutral'][['sentiment_category','sentiment']].copy()\n",
    "sentiment_PN['adj_sentiment'] = sentiment_PN['sentiment'].apply(abs)\n",
    "\n",
    "# Plotting histogram\n",
    "sns.histplot(data=sentiment_PN,\n",
    "             x=\"adj_sentiment\",\n",
    "            hue=\"sentiment_category\")\n",
    "plt.xlabel('Sentiments Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentiments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_sentiment_counts =  Yt_comments_eng_cms[Yt_comments_eng_cms['sentiment_category']!='neutral'].groupby(['topic_tag', 'sentiment_category']).size().reset_index(name='counts').unstack()\n",
    "\n",
    "plt.figure(figsize=(6, 4))  \n",
    "sns.barplot(x='counts', \n",
    "            y='topic_tag', \n",
    "            hue='sentiment_category', \n",
    "            data=topic_sentiment_counts, \n",
    "            palette=['#ee854a','#4878d0'],orient = 'h')\n",
    "\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xticks(rotation=70)\n",
    "plt.xlabel('count of comments')\n",
    "plt.ylabel('Topic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef4501",
   "metadata": {},
   "source": [
    "### 4.4 Research Question 3 What does the distribution of topics and sentiments (RQ2) imply about the primary audience reactions to Mr Beast's videos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From KL divergence find best perplexity\n",
    "from sklearn.manifold import TSNE\n",
    "W = nmf_topics\n",
    "perplexity = [50,100,200,300,400]\n",
    "divergence = []\n",
    "for i in perplexity:\n",
    "    model = TSNE(n_components=2, init=\"pca\", perplexity=i)\n",
    "    reduced = model.fit_transform(W)\n",
    "    divergence.append(model.kl_divergence_)\n",
    "KL_record = pd.DataFrame({\"perplexity\":perplexity,'divergence':divergence})\n",
    "KL_record.to_csv(\"./KL_record.csv\")\n",
    "KL_record = pd.read_csv(\"./KL_record.csv\")\n",
    "plt.plot(KL_record['perplexity'], KL_record['divergence'], marker='*',color='red')\n",
    "plt.title('KL Divergence metric')\n",
    "plt.grid()\n",
    "plt.xlabel('Perplexity')\n",
    "plt.ylabel('Divergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e617c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "W = nmf_topics\n",
    "\n",
    "# 2. Apply t-SNE\n",
    "tsne_200 = TSNE(n_components=2, \n",
    "                perplexity=200,\n",
    "                random_state=1) \n",
    " \n",
    "W_tsne200 = tsne_200.fit_transform(W)\n",
    "\n",
    "# 3. Create Scatter Plot with Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "df = pd.DataFrame(dict(x=W_tsne200[:,0],\n",
    "                       y=W_tsne200[:,1],\n",
    "                       label=np.argmax(W, axis=1)))\n",
    "\n",
    "groups = df.groupby('label')\n",
    "\n",
    "# Plot each group in a different color\n",
    "for name, group in groups:\n",
    "    plt.plot(group.x, \n",
    "             group.y,marker='.',\n",
    "             markersize=2,linestyle='', label=name)\n",
    "\n",
    "# Add topic labels\n",
    "xy_1 = 80\n",
    "xy_2 = (-15)\n",
    "for i, topic_num in enumerate(np.unique(np.argmax(W, axis=1))):\n",
    "    x = W_tsne200[np.argmax(W, axis=1) == topic_num, 0].mean()\n",
    "    y = W_tsne200[np.argmax(W, axis=1) == topic_num, 1].mean()\n",
    "    plt.text(x, y, f\"{topic_num}\", fontsize=15,bbox = dict(facecolor = 'yellow', alpha = 0.8))\n",
    "    xy_2 -= 5 \n",
    "    plt.annotate(f' ({topic_num}) {topic_annotation[topic_num]}',xy=[xy_1,xy_2])\n",
    "# Add legend and titles\n",
    "plt.legend(title='Topics', bbox_to_anchor=(1.15, 1), loc='upper right',markerscale=12)\n",
    "plt.annotate(' Topic:',xy=[xy_1,-15])\n",
    "plt.title('t-SNE Visualisation of NMF Topics')\n",
    "plt.grid()\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
